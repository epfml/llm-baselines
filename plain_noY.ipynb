{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yunyao/conda/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50257\n",
      "Tokenizing Shakespeare texts\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (338025 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num train data 80 percent: 1056, num val data 20 persent: 264, num tokens 338025 floor divided by max_seq_length 256\n",
      "train dataset size: 1056, val dataset size: 264\n"
     ]
    }
   ],
   "source": [
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    ")\n",
    "import os\n",
    "import requests\n",
    "import numpy as np\n",
    "import inspect\n",
    "# detect cuda\n",
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "class Args:\n",
    "    def __init__(self, **kwargs):\n",
    "        for key, value in kwargs.items():\n",
    "            setattr(self, key, value)\n",
    "\n",
    "# Example usage\n",
    "args = Args(\n",
    "    lr=1e-4, \n",
    "    beta1=0.9,\n",
    "    beta2=0.95,\n",
    "    weight_decay=0.01,\n",
    "    warmup_percent=0.05,\n",
    "    scheduler = 'cos',\n",
    "    batch_size=128, \n",
    "    num_epochs=30,\n",
    "    eval_freq =10,\n",
    "    device='cuda:0',\n",
    "    model_name='gpt2',\n",
    "    max_seq_length=256,\n",
    "    prompt = \"I would like to\",\n",
    ")\n",
    "\n",
    "device_type = \"cuda\" if \"cuda\" in str(args.device) else \"cpu\"\n",
    "if device_type == \"cuda\":\n",
    "    torch.cuda.set_device(args.device)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(args.model_name)\n",
    "model.to(device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(args.model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "max_seq_length = min(tokenizer.model_max_length, args.max_seq_length)\n",
    "print(tokenizer.vocab_size)\n",
    "\n",
    "def get_shakespeare_dataset(max_seq_length=max_seq_length):\n",
    "    char_tknzr = tokenizer.encode\n",
    "    DATA_PATH = os.path.join(os.getcwd(), \"datasets\", \"shakespeare\")\n",
    "    raw_path = os.path.join(DATA_PATH, \"raw.txt\")\n",
    "    train_path = os.path.join(DATA_PATH, f\"train.npy\")\n",
    "    test_path = os.path.join(DATA_PATH, f\"test.npy\")\n",
    "    # if path is not even there, download all data\n",
    "    if not os.path.exists(DATA_PATH):\n",
    "        print(\"Downloading raw Shakespeare texts\")\n",
    "        url = \"https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\"\n",
    "        os.makedirs(DATA_PATH, exist_ok=True)\n",
    "        text = requests.get(url, timeout=60).text\n",
    "        with open(raw_path, \"w+\", encoding=\"utf8\") as f:\n",
    "            f.write(text)\n",
    "    \n",
    "    if not os.path.exists(train_path) or not os.path.exists(test_path):\n",
    "        print(\"Tokenizing Shakespeare texts\")\n",
    "        # load text\n",
    "        with open(raw_path, encoding=\"utf8\") as f:\n",
    "            text = \"\".join(f.readlines())\n",
    "        # encode text\n",
    "        x_all = np.array(char_tknzr(text))\n",
    "        len_x_all = len(x_all)\n",
    "        seq = []\n",
    "        for i in range(len_x_all // max_seq_length):\n",
    "            x = x_all[i*max_seq_length:(i+1)*max_seq_length]\n",
    "            seq.append(x)\n",
    "        \n",
    "        indices = np.random.permutation(len(seq))\n",
    "        seq_shuffled = [seq[i] for i in indices]\n",
    "        train = seq_shuffled[:int(0.8*len(seq))]\n",
    "        val = seq_shuffled[int(0.8*len(seq)):]\n",
    "        # mem = np.memmap(train_path, dtype=np.uint16, mode=\"w+\", shape=(len(x_seq_train), max_seq_length))\n",
    "        # for i, x in enumerate(x_seq_train):\n",
    "        #     mem[i] = x\n",
    "        # mem = np.memmap(test_path, dtype=np.uint16, mode=\"w+\", shape=(len(x_seq_test), max_seq_length))\n",
    "        # for i, x in enumerate(x_seq_test):\n",
    "        #     mem[i] = x\n",
    "    print(f'num train data 80 percent: {len(train)}, num val data 20 persent: {len(val)}, num tokens {len(x_all)} floor divided by max_seq_length {max_seq_length}')\n",
    "    \n",
    "\n",
    "    return {\"train\": train, \"val\": val, \"shuffle\": indices}\n",
    "\n",
    "        # x = np.array(char_tknzr(text[:i]), dtype=np.uint16)\n",
    "        # x_test = np.array(char_tknzr(text[i:]), dtype=np.uint16)\n",
    "        # # map memory\n",
    "        # mem = np.memmap(train_path, dtype=np.uint16, mode=\"w+\", shape=x.shape)\n",
    "        # mem[:] = x\n",
    "        # mem = np.memmap(test_path, dtype=np.uint16, mode=\"w+\", shape=x_test.shape)\n",
    "        # mem[:] = x_test\n",
    "\n",
    "    # # at this point we know that the binfile was properly created so we load it\n",
    "    # return {\"train\": np.memmap(train_path, dtype=np.uint16, mode=\"r\"),\n",
    "    #         \"val\": np.memmap(test_path, dtype=np.uint16, mode=\"r\"),\n",
    "    #         \"shuffle\": indices}\n",
    "class MyDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, data):\n",
    "        super().__init__()\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        # chunk the data into sequences of length `sequence_length`\n",
    "        # NOTE: we discard the last remainding sequence if it's not of length `sequence_length`\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.data[idx]\n",
    "        return sample\n",
    "\n",
    "dataset = get_shakespeare_dataset(max_seq_length=args.max_seq_length)\n",
    "train_dataset = MyDataset(dataset['train'])# sft_config = SFTConfig(\n",
    "val_dataset = MyDataset(dataset['val'])\n",
    "\n",
    "print(f\"train dataset size: {len(train_dataset)}, val dataset size: {len(val_dataset)}\")\n",
    "#     dataset_text_field=\"text\",\n",
    "#     max_seq_length=512,\n",
    "#     output_dir=\"/tmp\",\n",
    "# )\n",
    "# trainer = SFTTrainer(\n",
    "#     \"gpt2\",\n",
    "#     train_dataset=dataset,\n",
    "#     args=sft_config,\n",
    "# )\n",
    "# trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num steps per epoch: 9\n",
      "num steps per val epoch: 3\n"
     ]
    }
   ],
   "source": [
    "train_loader = torch.utils.data.DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=args.batch_size,\n",
    "        shuffle=False,\n",
    "    )\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=args.batch_size,\n",
    "        shuffle=False,\n",
    "    )\n",
    "\n",
    "print(f'num steps per epoch: {len(train_loader)}')\n",
    "print(f'num steps per val epoch: {len(val_loader)}')\n",
    "# data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I would like to share with you some great tips and tricks that can help you out with your own build process.\n",
      "\n",
      "Here are some of the\n"
     ]
    }
   ],
   "source": [
    "\n",
    "input_ids = tokenizer(args.prompt, return_tensors=\"pt\").input_ids.to(device)\n",
    "gen_tokens = model.generate(\n",
    "    input_ids,\n",
    "    do_sample=True,\n",
    "    temperature=0.9,\n",
    "    max_length=30,\n",
    "    pad_token_id=tokenizer.eos_token_id  # EOS Token\n",
    ")\n",
    "gen_text = tokenizer.batch_decode(gen_tokens)[0]\n",
    "print(gen_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "use_fused = (device_type == 'cuda') and ('fused' in inspect.signature(torch.optim.AdamW).parameters)\n",
    "extra_args = dict(fused=True) if use_fused else dict()\n",
    "opt = torch.optim.AdamW(model.parameters(), lr=args.lr, betas=(args.beta1, args.beta2),\n",
    "                                weight_decay=args.weight_decay, **extra_args)\n",
    "\n",
    "iterations = len(train_loader) * args.num_epochs\n",
    "scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer=opt, max_lr=args.lr, \n",
    "                                                total_steps=iterations, \n",
    "                                                pct_start=args.warmup_percent, \n",
    "                                                anneal_strategy=args.scheduler, \n",
    "                                                cycle_momentum=False, div_factor=1e2, \n",
    "                                                final_div_factor=.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/0 [train] loss=4.600 [val] loss=4.427, acc=0.331485 [lr] 0.00000\n",
      "I would like to congratulate Toretto on his achievement in this monumental challenge.\"\n",
      "\n",
      "\"I think we have made a fantastic performance. I'm\n",
      "0/0 [train] loss=4.021 [val] loss=3.803, acc=0.339440 [lr] 0.00009\n",
      "I would like to thank all who, for their hospitality, and thank my dear neighbour, have remained steadfast, since the time of your parting, when\n",
      "0/0 [train] loss=3.802 [val] loss=3.642, acc=0.354831 [lr] 0.00010\n",
      "I would like to thank you, my children, for your hospitality and honourable service to us.\n",
      "\n",
      "You look, my brothers; you cannot\n",
      "0/0 [train] loss=3.648 [val] loss=3.548, acc=0.366105 [lr] 0.00010\n",
      "I would like to thank you gentlemen for this gracious deed!\n",
      "KING RICHARD II:\n",
      "Sir, I am your lord.\n",
      "\n",
      "KING\n",
      "0/0 [train] loss=3.546 [val] loss=3.485, acc=0.375368 [lr] 0.00010\n",
      "I would like to have done with you when I did come to thy house\n",
      "And I tell him, thou young man, how much I can't\n",
      "0/0 [train] loss=3.469 [val] loss=3.443, acc=0.382659 [lr] 0.00010\n",
      "I would like to set your hands on your bed;\n",
      "I might not not be so much for your mother's sake\n",
      "As for mine.\n",
      "\n",
      "0/0 [train] loss=3.412 [val] loss=3.420, acc=0.386152 [lr] 0.00009\n",
      "I would like to live,\n",
      "And have no more ado about than those where God would take\n",
      "You.\n",
      "PROSPERO:\n",
      "No\n",
      "0/0 [train] loss=3.367 [val] loss=3.402, acc=0.388399 [lr] 0.00009\n",
      "I would like to pray and to hear\n",
      "A new age about the marriage: for he's to be married;\n",
      "For now, for I want\n",
      "0/0 [train] loss=3.321 [val] loss=3.390, acc=0.390482 [lr] 0.00009\n",
      "I would like to thank the Lord as well as myself\n",
      "For this privilege of mine.\n",
      "\n",
      "BENVOLIO:\n",
      "So your royal majesty\n",
      "0/0 [train] loss=3.275 [val] loss=3.381, acc=0.391953 [lr] 0.00008\n",
      "I would like to be a guest at my house:\n",
      "My brother loves her very much, I must say;\n",
      "As for her mother, she\n",
      "0/0 [train] loss=3.241 [val] loss=3.373, acc=0.393627 [lr] 0.00008\n",
      "I would like to speak of this,\n",
      "That I shall not omit: what we shall call it,\n",
      "As far from this we shall not call\n",
      "0/0 [train] loss=3.214 [val] loss=3.365, acc=0.394261 [lr] 0.00008\n",
      "I would like to say what I have said before,\n",
      "While I am not the subject of it:\n",
      "I am not your lady\n",
      "Your friend\n",
      "0/0 [train] loss=3.178 [val] loss=3.365, acc=0.395282 [lr] 0.00007\n",
      "I would like to hear, and speak what the man is thinking.\n",
      "Sirius, it lies in thy heart,\n",
      "To set thee on the\n",
      "0/0 [train] loss=3.145 [val] loss=3.367, acc=0.395108 [lr] 0.00007\n",
      "I would like to be you,\n",
      "But will it please thee to take care\n",
      "That you be not so long as I, or your eyes,\n",
      "0/0 [train] loss=3.121 [val] loss=3.364, acc=0.396446 [lr] 0.00006\n",
      "I would like to know your reason,\n",
      "That I could tell you what was in your mind;\n",
      "And, that is, where you stand now\n",
      "0/0 [train] loss=3.098 [val] loss=3.367, acc=0.396640 [lr] 0.00006\n",
      "I would like to have,\n",
      "But I have not been here awhile.\n",
      "\n",
      "BRUTUS:\n",
      "I was once here, sir.\n",
      "\n",
      "0/0 [train] loss=3.080 [val] loss=3.370, acc=0.396558 [lr] 0.00005\n",
      "I would like to thank you.\n",
      "\n",
      "FLORIZEL:\n",
      "I have deserved death for my sins.\n",
      "\n",
      "MONTAGUE:\n",
      "\n",
      "0/0 [train] loss=3.055 [val] loss=3.372, acc=0.397968 [lr] 0.00005\n",
      "I would like to thank you.\n",
      "\n",
      "DUKE VINCENTIO:\n",
      "I had rather be obdurate than sit upon thy knee\n",
      "0/0 [train] loss=3.039 [val] loss=3.377, acc=0.398468 [lr] 0.00004\n",
      "I would like to speak with you, a prince.\n",
      "\n",
      "QUEEN MARGARET:\n",
      "I am glad you have come: would\n",
      "0/0 [train] loss=3.019 [val] loss=3.374, acc=0.398999 [lr] 0.00004\n",
      "I would like to speak.\n",
      "\n",
      "First Senator:\n",
      "I do beseech your majesty, my lordship, to\n",
      "answer me.\n",
      "\n",
      "0/0 [train] loss=3.006 [val] loss=3.374, acc=0.400082 [lr] 0.00003\n",
      "I would like to hear the reason of this.\n",
      "\n",
      "LEONTES:\n",
      "Sir,\n",
      "I am sorry, and would you be so,\n",
      "0/0 [train] loss=2.996 [val] loss=3.378, acc=0.399306 [lr] 0.00003\n",
      "I would like to go.\n",
      "I am in a state of joy. I do hate to hear it so:\n",
      "It is too good a time\n",
      "0/0 [train] loss=2.990 [val] loss=3.382, acc=0.399714 [lr] 0.00003\n",
      "I would like to have you, sir, you two, for the same purpose;\n",
      "And now for a knighthood wherefore you shall sit\n",
      "0/0 [train] loss=2.963 [val] loss=3.385, acc=0.398683 [lr] 0.00002\n",
      "I would like to lay hold of thee in prayers.\n",
      "\n",
      "BAPTISTA:\n",
      "Tut, if aught so offend thy nature\n",
      "0/0 [train] loss=2.961 [val] loss=3.384, acc=0.398805 [lr] 0.00002\n",
      "I would like to have another man.\n",
      "The mayor, sir, I think you have some warrants for my daughter.\n",
      "\n",
      "ROMEO:\n",
      "\n",
      "0/0 [train] loss=2.959 [val] loss=3.385, acc=0.398713 [lr] 0.00002\n",
      "I would like to know as we lie in wait,\n",
      "Which way we shall ascend and make our way.\n",
      "\n",
      "MENENIUS:\n",
      "\n",
      "0/0 [train] loss=2.946 [val] loss=3.389, acc=0.397927 [lr] 0.00001\n",
      "I would like to speak with thee; if thou didst, bid me so pardon thee\n",
      "As I have done and pray thee pardon me.\n",
      "\n",
      "0/0 [train] loss=2.944 [val] loss=3.389, acc=0.398529 [lr] 0.00001\n",
      "I would like to be a courtier; and one that cares not to be a judge\n",
      "Of law.\n",
      "\n",
      "ROMEO:\n",
      "If you\n",
      "0/0 [train] loss=2.948 [val] loss=3.389, acc=0.398407 [lr] 0.00001\n",
      "I would like to thank you, sir,\n",
      "For your kind and kind service, and for your good looks.\n",
      "\n",
      "MERCUTIO:\n",
      "0/0 [train] loss=2.939 [val] loss=3.391, acc=0.398611 [lr] 0.00001\n",
      "I would like to know how she fares.\n",
      "\n",
      "DUKE VINCENTIO:\n",
      "Well, what!\n",
      "\n",
      "ISABELLA\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from torch.nn import functional as F\n",
    "for epoch in range(args.num_epochs):\n",
    "    model.train()\n",
    "    for step_id, x in enumerate(train_loader):\n",
    "        # print(f'x shape: {x.shape}, y shape: {y.shape}')\n",
    "        x = x.to(device)\n",
    "        opt.zero_grad()\n",
    "        outputs = model(x, labels=x)\n",
    "        loss = outputs.loss\n",
    "        train_loss = loss.item()\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        scheduler.step()\n",
    "        opt.zero_grad()\n",
    "\n",
    "        if step_id % args.eval_freq == 0 or step_id == len(train_loader):\n",
    "            # Validation\n",
    "            model.eval()\n",
    "            epoch = step_id//len(train_loader)\n",
    "            current_lr = scheduler.get_last_lr()[0] if args.scheduler is not None else extra_args.lr\n",
    "                \n",
    "            correct_predictions = 0\n",
    "            total_predictions = 0\n",
    "            val_loss_sum = 0\n",
    "            val_loss_list = []  \n",
    "            val_acc_list = []\n",
    "            num_predictions = args.batch_size * args.max_seq_length\n",
    "            with torch.no_grad():\n",
    "                for x_val in val_loader:\n",
    "                    x_val = x_val.to(device)\n",
    "                    val_outputs = model(x_val, labels=x_val)\n",
    "                    # val_loss_list.append(val_outputs.loss)\n",
    "                    # val_loss_sum += val_outputs.loss.item()\n",
    "\n",
    "                    # Calculate token-level accuracy\n",
    "                    logits = val_outputs.logits\n",
    "                    shift_logits = logits[..., :-1, :].contiguous()\n",
    "                    shift_labels = x_val[..., 1:].contiguous()\n",
    "                    # Flatten the tokens\n",
    "                    loss = F.cross_entropy(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))\n",
    "                    # print(f'logits shape: {logits.shape}')\n",
    "                    predictions = torch.argmax(shift_logits, dim=-1)\n",
    "                    # print(f'predictions shape: {predictions.shape}')\n",
    "                    acc = (predictions == shift_labels).float().mean()\n",
    "                    val_loss_list.append(loss)\n",
    "                    val_acc_list.append(acc)\n",
    "                    # print(f'correct predictions: {correct_predictions}')\n",
    "                    # total_predictions += torch.numel(x)\n",
    "                    # print(f'total predictions: {total_predictions}')\n",
    "\n",
    "            # val_loss = val_loss_sum / len(val_loader)\n",
    "            # val_loss = sum(val_loss_list)/len(val_loss_list)\n",
    "            # print(f'val loss: {val_loss}')\n",
    "            val_acc = torch.stack(val_acc_list).mean().item()\n",
    "            val_loss = torch.stack(val_loss_list).mean().item()\n",
    "            # val_acc = sum(val_acc_list)/len(val_acc_list)\n",
    "            # print(f'val acc: {val_acc}')\n",
    "\n",
    "            print_string = f\"{epoch}/{step_id + epoch * len(train_loader)} [train] loss={train_loss:.3f} [val] loss={val_loss:.3f}, acc={val_acc:3f}\"\n",
    "            if scheduler is not None:\n",
    "                print_string += f\" [lr] {current_lr:.5f}\"\n",
    "            print(print_string)\n",
    "            input_ids = tokenizer(args.prompt, return_tensors=\"pt\").input_ids.to(device)\n",
    "            gen_tokens = model.generate(\n",
    "                input_ids,\n",
    "                do_sample=True,\n",
    "                temperature=0.9,\n",
    "                max_length=30,\n",
    "                pad_token_id=tokenizer.eos_token_id  # EOS Token\n",
    "            )\n",
    "            gen_text = tokenizer.batch_decode(gen_tokens)[0]\n",
    "            print(gen_text)\n",
    "\n",
    "        model.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n",
      "tensor([10, 11, 12, 13, 14, 15, 16, 17, 18, 19])\n",
      "tensor([20, 21, 22, 23, 24, 25, 26, 27, 28, 29])\n",
      "tensor([30, 31, 32, 33, 34, 35, 36, 37, 38, 39])\n",
      "tensor([40, 41, 42, 43, 44, 45, 46, 47, 48, 49])\n",
      "tensor([50, 51, 52, 53, 54, 55, 56, 57, 58, 59])\n",
      "tensor([60, 61, 62, 63, 64, 65, 66, 67, 68, 69])\n",
      "tensor([70, 71, 72, 73, 74, 75, 76, 77, 78, 79])\n",
      "tensor([80, 81, 82, 83, 84, 85, 86, 87, 88, 89])\n",
      "tensor([90, 91, 92, 93, 94, 95, 96, 97, 98, 99])\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader, SequentialSampler\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "# Example dataset\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.data[index]\n",
    "\n",
    "# Creating a sample dataset\n",
    "data = list(range(100))\n",
    "dataset = MyDataset(data)\n",
    "\n",
    "# DistributedSampler usage\n",
    "sampler = SequentialSampler(\n",
    "    dataset\n",
    ")\n",
    "\n",
    "# DataLoader with the DistributedSampler\n",
    "dataloader = DataLoader(dataset, sampler=sampler, batch_size=10)\n",
    "\n",
    "# Simulating data loading in a distributed setup\n",
    "for batch in dataloader:\n",
    "    print(batch)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
