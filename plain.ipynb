{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading raw Shakespeare texts\n",
      "Tokenizing Shakespeare texts\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (338025 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num train data 80 percent: 1056, num val data 20 persent: 264, num tokens 338025 floor divided by max_seq_length 256\n",
      "train dataset size: 1056, val dataset size: 264\n"
     ]
    }
   ],
   "source": [
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    DataCollator,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    PreTrainedModel,\n",
    "    PreTrainedTokenizerBase,\n",
    "    Trainer,\n",
    ")\n",
    "import os\n",
    "import requests\n",
    "import numpy as np\n",
    "import inspect\n",
    "# detect cuda\n",
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "class Args:\n",
    "    def __init__(self, **kwargs):\n",
    "        for key, value in kwargs.items():\n",
    "            setattr(self, key, value)\n",
    "\n",
    "# Example usage\n",
    "args = Args(\n",
    "    lr=1e-4, \n",
    "    beta1=0.9,\n",
    "    beta2=0.95,\n",
    "    weight_decay=0.1,\n",
    "    warmup_percent=0.05,\n",
    "    scheduler = 'cos',\n",
    "    batch_size=32, \n",
    "    num_epochs=3,\n",
    "    eval_freq = 1,\n",
    "    device='cuda:0',\n",
    "    model_name='gpt2',\n",
    "    max_seq_length=256,\n",
    "    prompt = \"I would like to\",\n",
    ")\n",
    "\n",
    "device_type = \"cuda\" if \"cuda\" in str(args.device) else \"cpu\"\n",
    "if device_type == \"cuda\":\n",
    "    torch.cuda.set_device(args.device)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(args.model_name)\n",
    "model.to(device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(args.model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "max_seq_length = min(tokenizer.model_max_length, args.max_seq_length)\n",
    "\n",
    "def get_shakespeare_dataset(max_seq_length=max_seq_length):\n",
    "    char_tknzr = tokenizer.encode\n",
    "    DATA_PATH = os.path.join(os.getcwd(), \"datasets\", \"shakespeare\")\n",
    "    raw_path = os.path.join(DATA_PATH, \"raw.txt\")\n",
    "    train_path = os.path.join(DATA_PATH, f\"train.npy\")\n",
    "    test_path = os.path.join(DATA_PATH, f\"test.npy\")\n",
    "    # if path is not even there, download all data\n",
    "    if not os.path.exists(DATA_PATH):\n",
    "        print(\"Downloading raw Shakespeare texts\")\n",
    "        url = \"https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\"\n",
    "        os.makedirs(DATA_PATH, exist_ok=True)\n",
    "        text = requests.get(url, timeout=60).text\n",
    "        with open(raw_path, \"w+\", encoding=\"utf8\") as f:\n",
    "            f.write(text)\n",
    "    \n",
    "    if not os.path.exists(train_path) or not os.path.exists(test_path):\n",
    "        print(\"Tokenizing Shakespeare texts\")\n",
    "        # load text\n",
    "        with open(raw_path, encoding=\"utf8\") as f:\n",
    "            text = \"\".join(f.readlines())\n",
    "        i = int(0.8*len(text))\n",
    "        # encode text\n",
    "        x_all = np.array(char_tknzr(text))\n",
    "        idx = 0\n",
    "        len_x_all = len(x_all)\n",
    "        x_seq = []\n",
    "        y_seq = []  \n",
    "        for i in range((len_x_all-1) // max_seq_length):\n",
    "            x = x_all[i*max_seq_length:(i+1)*max_seq_length]\n",
    "            y = x_all[i*max_seq_length+1:(i+1)*max_seq_length+1]\n",
    "            x_seq.append(x)\n",
    "            y_seq.append(y)\n",
    "        \n",
    "        indices = np.random.permutation(len(x_seq))\n",
    "        x_seq_shuffled = [x_seq[i] for i in indices]\n",
    "        y_seq_shuffled = [y_seq[i] for i in indices]\n",
    "        train_x, train_y = x_seq_shuffled[:int(0.8*len(x_seq))], y_seq_shuffled[:int(0.8*len(x_seq))]\n",
    "        val_x, val_y = x_seq_shuffled[int(0.8*len(x_seq)):], y_seq_shuffled[int(0.8*len(x_seq)):]\n",
    "        # mem = np.memmap(train_path, dtype=np.uint16, mode=\"w+\", shape=(len(x_seq_train), max_seq_length))\n",
    "        # for i, x in enumerate(x_seq_train):\n",
    "        #     mem[i] = x\n",
    "        # mem = np.memmap(test_path, dtype=np.uint16, mode=\"w+\", shape=(len(x_seq_test), max_seq_length))\n",
    "        # for i, x in enumerate(x_seq_test):\n",
    "        #     mem[i] = x\n",
    "    print(f'num train data 80 percent: {len(train_x)}, num val data 20 persent: {len(val_x)}, num tokens {len(x_all)} floor divided by max_seq_length {max_seq_length}')\n",
    "    \n",
    "\n",
    "    return {\"train_x\": train_x, \"train_y\": train_y, \"val_x\": val_x, \"val_y\": val_y, \"shuffle\": indices}\n",
    "\n",
    "        # x = np.array(char_tknzr(text[:i]), dtype=np.uint16)\n",
    "        # x_test = np.array(char_tknzr(text[i:]), dtype=np.uint16)\n",
    "        # # map memory\n",
    "        # mem = np.memmap(train_path, dtype=np.uint16, mode=\"w+\", shape=x.shape)\n",
    "        # mem[:] = x\n",
    "        # mem = np.memmap(test_path, dtype=np.uint16, mode=\"w+\", shape=x_test.shape)\n",
    "        # mem[:] = x_test\n",
    "\n",
    "    # # at this point we know that the binfile was properly created so we load it\n",
    "    # return {\"train\": np.memmap(train_path, dtype=np.uint16, mode=\"r\"),\n",
    "    #         \"val\": np.memmap(test_path, dtype=np.uint16, mode=\"r\"),\n",
    "    #         \"shuffle\": indices}\n",
    "class MyDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, data, labels):\n",
    "        super().__init__()\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        # chunk the data into sequences of length `sequence_length`\n",
    "        # NOTE: we discard the last remainding sequence if it's not of length `sequence_length`\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.data[idx]\n",
    "        label = self.labels[idx]\n",
    "        return sample, label\n",
    "\n",
    "dataset = get_shakespeare_dataset(max_seq_length=max_seq_length)\n",
    "train_dataset = MyDataset(dataset['train_x'], dataset['train_y'])# sft_config = SFTConfig(\n",
    "val_dataset = MyDataset(dataset['val_x'], dataset['val_y'])\n",
    "\n",
    "print(f\"train dataset size: {len(train_dataset)}, val dataset size: {len(val_dataset)}\")\n",
    "#     dataset_text_field=\"text\",\n",
    "#     max_seq_length=512,\n",
    "#     output_dir=\"/tmp\",\n",
    "# )\n",
    "# trainer = SFTTrainer(\n",
    "#     \"gpt2\",\n",
    "#     train_dataset=dataset,\n",
    "#     args=sft_config,\n",
    "# )\n",
    "# trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num steps per epoch: 17\n",
      "num steps per val epoch: 5\n"
     ]
    }
   ],
   "source": [
    "train_loader = torch.utils.data.DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=args.batch_size,\n",
    "        shuffle=False,\n",
    "    )\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=args.batch_size,\n",
    "        shuffle=False,\n",
    "    )\n",
    "\n",
    "print(f'num steps per epoch: {len(train_loader)}')\n",
    "print(f'num steps per val epoch: {len(val_loader)}')\n",
    "# data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I would like to apologize to the United States of America for disappointing and embarrassing us for having a strong position against the spread of such information and for putting\n"
     ]
    }
   ],
   "source": [
    "\n",
    "input_ids = tokenizer(args.prompt, return_tensors=\"pt\").input_ids.to(device)\n",
    "gen_tokens = model.generate(\n",
    "    input_ids,\n",
    "    do_sample=True,\n",
    "    temperature=0.9,\n",
    "    max_length=30,\n",
    "    pad_token_id=tokenizer.eos_token_id  # EOS Token\n",
    ")\n",
    "gen_text = tokenizer.batch_decode(gen_tokens)[0]\n",
    "print(gen_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "use_fused = (device_type == 'cuda') and ('fused' in inspect.signature(torch.optim.AdamW).parameters)\n",
    "extra_args = dict(fused=True) if use_fused else dict()\n",
    "opt = torch.optim.AdamW(model.parameters(), lr=args.lr, betas=(args.beta1, args.beta2),\n",
    "                                weight_decay=args.weight_decay, **extra_args)\n",
    "\n",
    "iterations = len(train_loader) * args.num_epochs\n",
    "scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer=opt, max_lr=args.lr, \n",
    "                                                total_steps=iterations, \n",
    "                                                pct_start=args.warmup_percent, \n",
    "                                                anneal_strategy=args.scheduler, \n",
    "                                                cycle_momentum=False, div_factor=1e2, \n",
    "                                                final_div_factor=.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/0 [train] loss=4.545 [val] loss=9.444, acc=0.329457 [lr] 0.00007\n",
      "I would like to add that I am a professional, well-heeled entrepreneur who would never let his money go to his personal expenses and in fact\n",
      "0/1 [train] loss=4.576 [val] loss=8.754, acc=0.318700 [lr] 0.00010\n",
      "I would like to thank the following people for this great opportunity:\n",
      "\n",
      "Curtis Smith (USOC)\n",
      "\n",
      "The Office of the Vice\n",
      "0/2 [train] loss=4.237 [val] loss=8.647, acc=0.314305 [lr] 0.00010\n",
      "I would like to thank those who have helped out with their hard work to support our clients. I hope you will join us for a long-term\n",
      "0/3 [train] loss=4.119 [val] loss=8.599, acc=0.321659 [lr] 0.00010\n",
      "I would like to thank everyone for your support. I am still not sure if there is much I wish I could do for you.\n",
      "\n",
      "Thanks\n",
      "0/4 [train] loss=4.088 [val] loss=8.607, acc=0.332579 [lr] 0.00010\n",
      "I would like to thank everyone for sharing their ideas and comments and for making this possible. I wish you all the best\n",
      "\n",
      "David\n",
      "\n",
      "A\n",
      "0/5 [train] loss=4.024 [val] loss=8.673, acc=0.331987 [lr] 0.00010\n",
      "I would like to say to my friend: Dear Sir,\n",
      "\n",
      "I am very much obliged you know. You have just had a pleasure,\n",
      "\n",
      "0/6 [train] loss=4.049 [val] loss=8.756, acc=0.335109 [lr] 0.00010\n",
      "I would like to call this a day before Christmas, where the great men of the world are gathered round in the town to hear him speak. He\n",
      "0/7 [train] loss=4.002 [val] loss=8.845, acc=0.336796 [lr] 0.00010\n",
      "I would like to know whether I can find it in any other form, if it be mine,\" he replied,\n",
      "that is, in the presence\n",
      "0/8 [train] loss=4.039 [val] loss=8.893, acc=0.340169 [lr] 0.00009\n",
      "I would like to invite you,\n",
      "The lady in whom I am a guest, to appear with, my husband,\n",
      "So to have his name\n",
      "0/9 [train] loss=3.912 [val] loss=8.899, acc=0.344579 [lr] 0.00009\n",
      "I would like to offer my sincere thanks to you for this.\n",
      "\n",
      "Thank you, I will tell you that I am one of the finest women\n",
      "0/10 [train] loss=3.882 [val] loss=8.879, acc=0.348071 [lr] 0.00009\n",
      "I would like to talk to you about the state of your health. I think you are very well; but I think that your life is very fine\n",
      "0/11 [train] loss=3.987 [val] loss=8.859, acc=0.349358 [lr] 0.00009\n",
      "I would like to call again, this is the end.\n",
      "\n",
      "DUKE:\n",
      "I was once in exile. I was on my knees\n",
      "0/12 [train] loss=3.755 [val] loss=8.844, acc=0.351548 [lr] 0.00009\n",
      "I would like to know, from whom comes our business, and from whence come the success of our work, that you and I, our two gentlemen\n",
      "0/13 [train] loss=3.887 [val] loss=8.834, acc=0.354803 [lr] 0.00009\n",
      "I would like to have the Lord and Creator with me.\n",
      "\n",
      "DUKE:\n",
      "What are you, a humble citizen?\n",
      "\n",
      "P\n",
      "0/14 [train] loss=3.782 [val] loss=8.845, acc=0.356771 [lr] 0.00008\n",
      "I would like to offer you in my behalf: I trust you shall receive him my honour: wherefore, therefore, I give you my consent.\n",
      "0/15 [train] loss=3.840 [val] loss=8.866, acc=0.357599 [lr] 0.00008\n",
      "I would like to present to you my name as well, brother.\n",
      "SOUTER: I am, your majesty,\n",
      "The son of your\n",
      "0/16 [train] loss=3.785 [val] loss=8.896, acc=0.358650 [lr] 0.00008\n",
      "I would like to thank the gentleman for his gracious presence.\n",
      "\n",
      "PENTOLIO:\n",
      "If you cannot be the guest,\n",
      "You are\n",
      "0/0 [train] loss=3.672 [val] loss=8.926, acc=0.360204 [lr] 0.00008\n",
      "I would like to give you a good reason for this request of my father; you must not be ashamed to make it yourself.\n",
      "\n",
      "\n",
      "W\n",
      "0/1 [train] loss=3.682 [val] loss=8.964, acc=0.362038 [lr] 0.00007\n",
      "I would like to thank you, for the great work you have done.\n",
      "\n",
      "To the RICHARD II:\n",
      "I must say something to\n",
      "0/2 [train] loss=3.703 [val] loss=9.009, acc=0.364376 [lr] 0.00007\n",
      "I would like to know what will be done for you?\n",
      "YORKBUCK:\n",
      "Go, O Lord, I bid thee pardon, and\n",
      "0/3 [train] loss=3.672 [val] loss=9.056, acc=0.365190 [lr] 0.00007\n",
      "I would like to be a lady of service and help my friends so as you know he hath left them.\n",
      "LUCENTIO:\n",
      "That\n",
      "0/4 [train] loss=3.696 [val] loss=9.094, acc=0.366610 [lr] 0.00007\n",
      "I would like to speak to you, whereupon, say I, he is dead.\n",
      "\n",
      "KING JOHN VINCENTIO:\n",
      "Marry\n",
      "0/5 [train] loss=3.669 [val] loss=9.117, acc=0.367365 [lr] 0.00006\n",
      "I would like to commend myself to your father; but since we are friends and in close connection, we shall not know whether he conspires to make\n",
      "0/6 [train] loss=3.699 [val] loss=9.126, acc=0.368194 [lr] 0.00006\n",
      "I would like to,\n",
      "And that would be done, as it were,\n",
      "For I would so make use of his kindness.\n",
      "\n",
      "M\n",
      "0/7 [train] loss=3.680 [val] loss=9.124, acc=0.368149 [lr] 0.00006\n",
      "I would like to be rid of you, my brother, but I cannot be.\n",
      "\n",
      "LEBERT:\n",
      "O, my lord, to\n",
      "0/8 [train] loss=3.738 [val] loss=9.116, acc=0.368312 [lr] 0.00005\n",
      "I would like to take the time to talk with you more intimately than I have been. I thank you kindly, sir, for that. I would\n",
      "0/9 [train] loss=3.635 [val] loss=9.111, acc=0.368741 [lr] 0.00005\n",
      "I would like to be your father; for this is my hope to save you.\n",
      "\n",
      "QUEEN:\n",
      "Lord, I have heard of\n",
      "0/10 [train] loss=3.630 [val] loss=9.105, acc=0.369466 [lr] 0.00005\n",
      "I would like to ask your pardon.\n",
      "\n",
      "Your lordship,\n",
      "I have given you such a request in relation to mine,\n",
      "That I\n",
      "0/11 [train] loss=3.754 [val] loss=9.098, acc=0.370517 [lr] 0.00005\n",
      "I would like to say a little more: I am afraid that all those who, like you, have a sense of common duty and that the common\n",
      "0/12 [train] loss=3.538 [val] loss=9.096, acc=0.371316 [lr] 0.00004\n",
      "I would like to have you and my friends. I have you and my friends, you and your friends, and so farewell:\n",
      "Here's my\n",
      "0/13 [train] loss=3.663 [val] loss=9.100, acc=0.372514 [lr] 0.00004\n",
      "I would like to thank Mr. Blaise, but you are not worthy of the title,\n",
      "That it is not a noble title. Come\n",
      "0/14 [train] loss=3.575 [val] loss=9.111, acc=0.372736 [lr] 0.00004\n",
      "I would like to go forward.\n",
      "BRUTUS:\n",
      "I know then;\n",
      "The noble prince of Troy,\n",
      "Of noble lords of Greece\n",
      "0/15 [train] loss=3.665 [val] loss=9.123, acc=0.373180 [lr] 0.00003\n",
      "I would like to hear the answer. I do not answer to him, I have never answered to him.\n",
      "\n",
      "MENENIUS:\n",
      "\n",
      "0/16 [train] loss=3.590 [val] loss=9.136, acc=0.373313 [lr] 0.00003\n",
      "I would like to speak to thee,\n",
      "I wish I could speak to thee,\n",
      "I am so dear to you, so loving and so blessed\n",
      "0/0 [train] loss=3.513 [val] loss=9.147, acc=0.373654 [lr] 0.00003\n",
      "I would like to do you a good deal,\n",
      "With whom I shall know my place, my heart, my mind, my manners!\n",
      "KING\n",
      "0/1 [train] loss=3.547 [val] loss=9.161, acc=0.373728 [lr] 0.00003\n",
      "I would like to hear this: it is your custom, your duty, your duty to marry at this moment:\n",
      "\n",
      "Yet, I have never\n",
      "0/2 [train] loss=3.582 [val] loss=9.176, acc=0.374083 [lr] 0.00003\n",
      "I would like to report a report of his cousin,\n",
      "I will make him my friend.\n",
      "\n",
      "KING RICHARD II:\n",
      "Well,\n",
      "0/3 [train] loss=3.544 [val] loss=9.194, acc=0.374127 [lr] 0.00002\n",
      "I would like to say that to their mother,\n",
      "And say that to their daughter, from the age of two.\n",
      "\n",
      "GLOUCES\n",
      "0/4 [train] loss=3.589 [val] loss=9.208, acc=0.374867 [lr] 0.00002\n",
      "I would like to thank you all. What's your business, then?\n",
      "\n",
      "SICINIUS:\n",
      "It is: here you are\n",
      "0/5 [train] loss=3.555 [val] loss=9.221, acc=0.375148 [lr] 0.00002\n",
      "I would like to say to you:\n",
      "We shall not hear: there is more fear on this side, and more hope in the other.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "for epoch in range(args.num_epochs):\n",
    "    model.train()\n",
    "    for step_id, (x, y) in enumerate(train_loader):\n",
    "        # print(f'x shape: {x.shape}, y shape: {y.shape}')\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "        opt.zero_grad()\n",
    "        outputs = model(x, labels=x)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        scheduler.step()\n",
    "        opt.zero_grad()\n",
    "\n",
    "        if step_id % args.eval_freq == 0 or step_id == len(train_loader):\n",
    "            # Validation\n",
    "            model.eval()\n",
    "            epoch = step_id//len(train_loader)\n",
    "\n",
    "            train_loss = loss.detach().cpu().item() \n",
    "            current_lr = scheduler.get_last_lr()[0] if args.scheduler is not None else extra_args.lr\n",
    "                \n",
    "            correct_predictions = 0\n",
    "            total_predictions = 0\n",
    "            val_loss = 0\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                for x, y in val_loader:\n",
    "                    x = x.to(device)\n",
    "                    y = y.to(device)\n",
    "                    val_outputs = model(x, labels=y)\n",
    "                    val_loss += val_outputs.loss.item()\n",
    "\n",
    "                    # Calculate token-level accuracy\n",
    "                    logits = val_outputs.logits\n",
    "                    predictions = torch.argmax(logits, dim=-1)\n",
    "                    correct_predictions += (predictions == y).sum().item()\n",
    "                    total_predictions += torch.numel(x)\n",
    "\n",
    "            val_loss = val_loss / len(val_loader)\n",
    "            val_acc = correct_predictions / total_predictions\n",
    "\n",
    "            print_string = f\"{epoch}/{step_id + epoch * len(train_loader)} [train] loss={train_loss:.3f} [val] loss={val_loss:.3f}, acc={val_acc:3f}\"\n",
    "            if scheduler is not None:\n",
    "                print_string += f\" [lr] {current_lr:.5f}\"\n",
    "            print(print_string)\n",
    "            input_ids = tokenizer(args.prompt, return_tensors=\"pt\").input_ids.to(device)\n",
    "            gen_tokens = model.generate(\n",
    "                input_ids,\n",
    "                do_sample=True,\n",
    "                temperature=0.9,\n",
    "                max_length=30,\n",
    "                pad_token_id=tokenizer.eos_token_id  # EOS Token\n",
    "            )\n",
    "            gen_text = tokenizer.batch_decode(gen_tokens)[0]\n",
    "            print(gen_text)\n",
    "\n",
    "            model.train()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
