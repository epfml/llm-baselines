{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yunyao/conda/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing Shakespeare texts\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (338025 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num train data 80 percent: 1056, num val data 20 persent: 264, num tokens 338025 floor divided by max_seq_length 256\n",
      "train dataset size: 1056, val dataset size: 264\n"
     ]
    }
   ],
   "source": [
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    DataCollator,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    PreTrainedModel,\n",
    "    PreTrainedTokenizerBase,\n",
    "    Trainer,\n",
    ")\n",
    "import os\n",
    "import requests\n",
    "import numpy as np\n",
    "import inspect\n",
    "# detect cuda\n",
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "class Args:\n",
    "    def __init__(self, **kwargs):\n",
    "        for key, value in kwargs.items():\n",
    "            setattr(self, key, value)\n",
    "\n",
    "# Example usage\n",
    "args = Args(\n",
    "    lr=1e-4, \n",
    "    beta1=0.9,\n",
    "    beta2=0.95,\n",
    "    weight_decay=0.01,\n",
    "    warmup_percent=0.05,\n",
    "    scheduler = 'cos',\n",
    "    batch_size=32, \n",
    "    num_epochs=3,\n",
    "    eval_freq = 2,\n",
    "    device='cuda:0',\n",
    "    model_name='gpt2',\n",
    "    max_seq_length=256,\n",
    "    prompt = \"I would like to\",\n",
    ")\n",
    "\n",
    "device_type = \"cuda\" if \"cuda\" in str(args.device) else \"cpu\"\n",
    "if device_type == \"cuda\":\n",
    "    torch.cuda.set_device(args.device)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(args.model_name)\n",
    "model.to(device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(args.model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "max_seq_length = min(tokenizer.model_max_length, args.max_seq_length)\n",
    "\n",
    "def get_shakespeare_dataset(max_seq_length=max_seq_length):\n",
    "    char_tknzr = tokenizer.encode\n",
    "    DATA_PATH = os.path.join(os.getcwd(), \"datasets\", \"shakespeare\")\n",
    "    raw_path = os.path.join(DATA_PATH, \"raw.txt\")\n",
    "    train_path = os.path.join(DATA_PATH, f\"train.npy\")\n",
    "    test_path = os.path.join(DATA_PATH, f\"test.npy\")\n",
    "    # if path is not even there, download all data\n",
    "    if not os.path.exists(DATA_PATH):\n",
    "        print(\"Downloading raw Shakespeare texts\")\n",
    "        url = \"https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\"\n",
    "        os.makedirs(DATA_PATH, exist_ok=True)\n",
    "        text = requests.get(url, timeout=60).text\n",
    "        with open(raw_path, \"w+\", encoding=\"utf8\") as f:\n",
    "            f.write(text)\n",
    "    \n",
    "    if not os.path.exists(train_path) or not os.path.exists(test_path):\n",
    "        print(\"Tokenizing Shakespeare texts\")\n",
    "        # load text\n",
    "        with open(raw_path, encoding=\"utf8\") as f:\n",
    "            text = \"\".join(f.readlines())\n",
    "        i = int(0.8*len(text))\n",
    "        # encode text\n",
    "        x_all = np.array(char_tknzr(text))\n",
    "        idx = 0\n",
    "        len_x_all = len(x_all)\n",
    "        x_seq = []\n",
    "        # y_seq = []  \n",
    "        for i in range(len_x_all // max_seq_length):\n",
    "            x = x_all[i*max_seq_length:(i+1)*max_seq_length]\n",
    "            # y = x_all[i*max_seq_length+1:(i+1)*max_seq_length+1]\n",
    "            x_seq.append(x)\n",
    "            # y_seq.append(y)\n",
    "        \n",
    "        indices = np.random.permutation(len(x_seq))\n",
    "        seq_shuffled = [x_seq[i] for i in indices]\n",
    "        # y_seq_shuffled = [y_seq[i] for i in indices]\n",
    "        train = seq_shuffled[:int(0.8*len(x_seq))]\n",
    "        val = seq_shuffled[int(0.8*len(x_seq)):]\n",
    "        # mem = np.memmap(train_path, dtype=np.uint16, mode=\"w+\", shape=(len(x_seq_train), max_seq_length))\n",
    "        # for i, x in enumerate(x_seq_train):\n",
    "        #     mem[i] = x\n",
    "        # mem = np.memmap(test_path, dtype=np.uint16, mode=\"w+\", shape=(len(x_seq_test), max_seq_length))\n",
    "        # for i, x in enumerate(x_seq_test):\n",
    "        #     mem[i] = x\n",
    "    print(f'num train data 80 percent: {len(train)}, num val data 20 persent: {len(val)}, num tokens {len(x_all)} floor divided by max_seq_length {max_seq_length}')\n",
    "    \n",
    "\n",
    "    return {\"train_x\": train_x, \"train_y\": train_y, \"val_x\": val_x, \"val_y\": val_y, \"shuffle\": indices}\n",
    "\n",
    "        # x = np.array(char_tknzr(text[:i]), dtype=np.uint16)\n",
    "        # x_test = np.array(char_tknzr(text[i:]), dtype=np.uint16)\n",
    "        # # map memory\n",
    "        # mem = np.memmap(train_path, dtype=np.uint16, mode=\"w+\", shape=x.shape)\n",
    "        # mem[:] = x\n",
    "        # mem = np.memmap(test_path, dtype=np.uint16, mode=\"w+\", shape=x_test.shape)\n",
    "        # mem[:] = x_test\n",
    "\n",
    "    # # at this point we know that the binfile was properly created so we load it\n",
    "    # return {\"train\": np.memmap(train_path, dtype=np.uint16, mode=\"r\"),\n",
    "    #         \"val\": np.memmap(test_path, dtype=np.uint16, mode=\"r\"),\n",
    "    #         \"shuffle\": indices}\n",
    "class MyDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, data, labels):\n",
    "        super().__init__()\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        # chunk the data into sequences of length `sequence_length`\n",
    "        # NOTE: we discard the last remainding sequence if it's not of length `sequence_length`\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.data[idx]\n",
    "        label = self.labels[idx]\n",
    "        return sample, label\n",
    "\n",
    "dataset = get_shakespeare_dataset(max_seq_length=args.max_seq_length)\n",
    "train_dataset = MyDataset(dataset['train_x'], dataset['train_y'])# sft_config = SFTConfig(\n",
    "val_dataset = MyDataset(dataset['val_x'], dataset['val_y'])\n",
    "\n",
    "print(f\"train dataset size: {len(train_dataset)}, val dataset size: {len(val_dataset)}\")\n",
    "#     dataset_text_field=\"text\",\n",
    "#     max_seq_length=512,\n",
    "#     output_dir=\"/tmp\",\n",
    "# )\n",
    "# trainer = SFTTrainer(\n",
    "#     \"gpt2\",\n",
    "#     train_dataset=dataset,\n",
    "#     args=sft_config,\n",
    "# )\n",
    "# trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num steps per epoch: 33\n",
      "num steps per val epoch: 9\n"
     ]
    }
   ],
   "source": [
    "train_loader = torch.utils.data.DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=args.batch_size,\n",
    "        shuffle=False,\n",
    "    )\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=args.batch_size,\n",
    "        shuffle=False,\n",
    "    )\n",
    "\n",
    "print(f'num steps per epoch: {len(train_loader)}')\n",
    "print(f'num steps per val epoch: {len(val_loader)}')\n",
    "# data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I would like to thank our donors, and our volunteers and our community, for working so hard.\n",
      "\n",
      "In the past ten years, we've\n"
     ]
    }
   ],
   "source": [
    "\n",
    "input_ids = tokenizer(args.prompt, return_tensors=\"pt\").input_ids.to(device)\n",
    "gen_tokens = model.generate(\n",
    "    input_ids,\n",
    "    do_sample=True,\n",
    "    temperature=0.9,\n",
    "    max_length=30,\n",
    "    pad_token_id=tokenizer.eos_token_id  # EOS Token\n",
    ")\n",
    "gen_text = tokenizer.batch_decode(gen_tokens)[0]\n",
    "print(gen_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "use_fused = (device_type == 'cuda') and ('fused' in inspect.signature(torch.optim.AdamW).parameters)\n",
    "extra_args = dict(fused=True) if use_fused else dict()\n",
    "opt = torch.optim.AdamW(model.parameters(), lr=args.lr, betas=(args.beta1, args.beta2),\n",
    "                                weight_decay=args.weight_decay, **extra_args)\n",
    "\n",
    "iterations = len(train_loader) * args.num_epochs\n",
    "scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer=opt, max_lr=args.lr, \n",
    "                                                total_steps=iterations, \n",
    "                                                pct_start=args.warmup_percent, \n",
    "                                                anneal_strategy=args.scheduler, \n",
    "                                                cycle_momentum=False, div_factor=1e2, \n",
    "                                                final_div_factor=.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/0 [train] loss=4.606 [val] loss=4.410, acc=0.323581 [lr] 0.00002\n",
      "I would like to know if anyone has any other thoughts on the matter: What do you think would have happened if you are currently in charge of The\n",
      "0/2 [train] loss=4.445 [val] loss=3.969, acc=0.316203 [lr] 0.00009\n",
      "I would like to speak with you about the situation.\n",
      "\n",
      "\n",
      "What are your thoughts on the recent situation, and why did you decide to go back\n",
      "0/4 [train] loss=4.093 [val] loss=3.909, acc=0.314019 [lr] 0.00010\n",
      "I would like to say thank you for doing so great work. I am so very privileged with the opportunity to be able to become your mentor here.\n",
      "0/6 [train] loss=3.977 [val] loss=3.846, acc=0.325534 [lr] 0.00010\n",
      "I would like to ask you to give me some assurance that I am fully willing to meet as many worthy men as I can bear in your midst,\n",
      "0/8 [train] loss=4.025 [val] loss=3.812, acc=0.330295 [lr] 0.00010\n",
      "I would like to hear your answer, it seems.\n",
      "\n",
      "How you say, \"I am not your father. I love my father,\" says\n",
      "0/10 [train] loss=4.087 [val] loss=3.786, acc=0.334188 [lr] 0.00010\n",
      "I would like to thank you for your assistance.\n",
      "\n",
      "NAPOLEUS\n",
      "\n",
      "Thank you, and thank you:\n",
      "POUL\n",
      "0/12 [train] loss=3.809 [val] loss=3.739, acc=0.339057 [lr] 0.00010\n",
      "I would like to thank his friends and the people who have sent him over.\n",
      "\n",
      "\n",
      "VINCENTINI:\n",
      "And now, I\n",
      "0/14 [train] loss=3.833 [val] loss=3.709, acc=0.342963 [lr] 0.00010\n",
      "I would like to thank you for doing so well, and for your care, it pains me at this time.\"\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "B\n",
      "0/16 [train] loss=3.790 [val] loss=3.681, acc=0.347765 [lr] 0.00010\n",
      "I would like to say, sir, that my words have already been said, and not to hear them till I think, and then may say my\n",
      "0/18 [train] loss=3.722 [val] loss=3.661, acc=0.349379 [lr] 0.00009\n",
      "I would like to thank You since it is such;\n",
      "I have made myself known to them; their business\n",
      "Which the world, that I will\n",
      "0/20 [train] loss=3.829 [val] loss=3.635, acc=0.352376 [lr] 0.00009\n",
      "I would like to know how much the state of his heart will allow us in this state, so that the general will be well acquainted with his state\n",
      "0/22 [train] loss=3.565 [val] loss=3.618, acc=0.354655 [lr] 0.00009\n",
      "I would like to give you such encouragement as I may for a few things.\n",
      "NAPPER:\n",
      "My dear mother, are you well to\n",
      "0/24 [train] loss=3.724 [val] loss=3.602, acc=0.357110 [lr] 0.00009\n",
      "I would like to thank you, O Lord, for this good news of mine heart:\n",
      "I have known you very well, and you are good\n",
      "0/26 [train] loss=3.740 [val] loss=3.587, acc=0.358643 [lr] 0.00009\n",
      "I would like to see you,\n",
      "The son of God,\n",
      "If you would like to be of good cheer to us;\n",
      "I mean you\n",
      "0/28 [train] loss=3.788 [val] loss=3.572, acc=0.360813 [lr] 0.00009\n",
      "I would like to say as much:\n",
      "I'll leave, I'll say if I had\n",
      "the courage to give my life with these words.\n",
      "0/30 [train] loss=3.641 [val] loss=3.560, acc=0.362739 [lr] 0.00008\n",
      "I would like to leave you to comfort me, and I am sure I shall.\n",
      "GISBY:\n",
      "If you had an hour, I\n",
      "0/32 [train] loss=3.770 [val] loss=3.548, acc=0.364434 [lr] 0.00008\n",
      "I would like to see your children.\n",
      "\n",
      "YORK:\n",
      "So, this is your daughter: you are a woman very virtuous.\n",
      "\n",
      "\n",
      "0/0 [train] loss=3.663 [val] loss=3.543, acc=0.365045 [lr] 0.00008\n",
      "I would like to make a present; I have so many good times\n",
      "To make this present,\n",
      "But with some slight alteration:\n",
      "I would\n",
      "0/2 [train] loss=3.637 [val] loss=3.533, acc=0.366713 [lr] 0.00008\n",
      "I would like to know what\n",
      "The Lord wills to do to me I'll tell unto thee.\n",
      "O, what means\n",
      "That I may leave\n",
      "0/4 [train] loss=3.489 [val] loss=3.525, acc=0.367906 [lr] 0.00007\n",
      "I would like to say I, the son of Josephine, with you,\n",
      "I beg you, that my daughter marry me to your father!\n",
      "0/6 [train] loss=3.483 [val] loss=3.517, acc=0.368910 [lr] 0.00007\n",
      "I would like to stay,\n",
      "And tell you something.\n",
      "\n",
      "QUEEN VINCENTIO:\n",
      "If I cannot speak, I'll\n",
      "0/8 [train] loss=3.575 [val] loss=3.507, acc=0.370660 [lr] 0.00007\n",
      "I would like to thank you: with your prayers and your grace to our hearts, our prayers and all the world's prayers to you, now be\n",
      "0/10 [train] loss=3.648 [val] loss=3.499, acc=0.371596 [lr] 0.00007\n",
      "I would like to hear from you yourself:\n",
      "Do you think he is the man?\n",
      "HORTENSIO:\n",
      "No, he is the\n",
      "0/12 [train] loss=3.421 [val] loss=3.496, acc=0.371731 [lr] 0.00006\n",
      "I would like to know the name of Edward Grey, and the power that he wields\n",
      "To fight the world.\n",
      "DUKE VINC\n",
      "0/14 [train] loss=3.487 [val] loss=3.491, acc=0.371989 [lr] 0.00006\n",
      "I would like to report you a little more: the man himself had his time,\n",
      "But he did so late as you did.\n",
      "\n",
      "H\n",
      "0/16 [train] loss=3.493 [val] loss=3.488, acc=0.372437 [lr] 0.00006\n",
      "I would like to know the meaning of this,\n",
      "And do you hear that the king hath said to thee?\n",
      "\n",
      "KING EDWARD VI:\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 12\u001b[0m\n\u001b[1;32m     10\u001b[0m loss \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mloss\n\u001b[1;32m     11\u001b[0m train_loss \u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n\u001b[0;32m---> 12\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m opt\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     14\u001b[0m scheduler\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[0;32m~/conda/lib/python3.12/site-packages/torch/_tensor.py:522\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    512\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    513\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    514\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    515\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    520\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    521\u001b[0m     )\n\u001b[0;32m--> 522\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    524\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/conda/lib/python3.12/site-packages/torch/autograd/__init__.py:266\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    261\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    263\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 266\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    267\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "from torch.nn import functional as F\n",
    "for epoch in range(args.num_epochs):\n",
    "    model.train()\n",
    "    for step_id, (x, y) in enumerate(train_loader):\n",
    "        # print(f'x shape: {x.shape}, y shape: {y.shape}')\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "        opt.zero_grad()\n",
    "        outputs = model(x, labels=x)\n",
    "        loss = outputs.loss\n",
    "        train_loss = loss.item()\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        scheduler.step()\n",
    "        opt.zero_grad()\n",
    "\n",
    "        if step_id % args.eval_freq == 0 or step_id == len(train_loader):\n",
    "            # Validation\n",
    "            model.eval()\n",
    "            epoch = step_id//len(train_loader)\n",
    "            current_lr = scheduler.get_last_lr()[0] if args.scheduler is not None else extra_args.lr\n",
    "                \n",
    "            correct_predictions = 0\n",
    "            total_predictions = 0\n",
    "            val_loss_sum = 0\n",
    "            val_loss_list = []  \n",
    "            val_acc_list = []\n",
    "            num_predictions = args.batch_size * args.max_seq_length\n",
    "            with torch.no_grad():\n",
    "                for x_val, y_val in val_loader:\n",
    "                    x_val = x_val.to(device)\n",
    "                    y_val = y_val.to(device)\n",
    "                    val_outputs = model(x_val, labels=y_val)\n",
    "                    # val_loss_list.append(val_outputs.loss)\n",
    "                    # val_loss_sum += val_outputs.loss.item()\n",
    "\n",
    "                    # Calculate token-level accuracy\n",
    "                    logits = val_outputs.logits\n",
    "                    # print(f'logits shape: {logits.shape}')\n",
    "                    predictions = torch.argmax(logits, dim=-1)\n",
    "                    # print(f'predictions shape: {predictions.shape}')\n",
    "                    acc = (predictions == y_val).float().mean()\n",
    "                    loss = F.cross_entropy(logits.view(-1, logits.size(-1)), y_val.view(-1))\n",
    "                    val_loss_list.append(loss)\n",
    "                    val_acc_list.append(acc)\n",
    "                    # print(f'correct predictions: {correct_predictions}')\n",
    "                    # total_predictions += torch.numel(x)\n",
    "                    # print(f'total predictions: {total_predictions}')\n",
    "\n",
    "            # val_loss = val_loss_sum / len(val_loader)\n",
    "            # val_loss = sum(val_loss_list)/len(val_loss_list)\n",
    "            # print(f'val loss: {val_loss}')\n",
    "            val_acc = torch.stack(val_acc_list).mean().item()\n",
    "            val_loss = torch.stack(val_loss_list).mean().item()\n",
    "            # val_acc = sum(val_acc_list)/len(val_acc_list)\n",
    "            # print(f'val acc: {val_acc}')\n",
    "\n",
    "            print_string = f\"{epoch}/{step_id + epoch * len(train_loader)} [train] loss={train_loss:.3f} [val] loss={val_loss:.3f}, acc={val_acc:3f}\"\n",
    "            if scheduler is not None:\n",
    "                print_string += f\" [lr] {current_lr:.5f}\"\n",
    "            print(print_string)\n",
    "            input_ids = tokenizer(args.prompt, return_tensors=\"pt\").input_ids.to(device)\n",
    "            gen_tokens = model.generate(\n",
    "                input_ids,\n",
    "                do_sample=True,\n",
    "                temperature=0.9,\n",
    "                max_length=30,\n",
    "                pad_token_id=tokenizer.eos_token_id  # EOS Token\n",
    "            )\n",
    "            gen_text = tokenizer.batch_decode(gen_tokens)[0]\n",
    "            print(gen_text)\n",
    "\n",
    "        model.train()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
